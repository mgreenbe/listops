{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dtype\n",
    "\n",
    "\n",
    "tokens = [*[str(i) for i in range(10)], \"MIN\", \"MAX\", \"MED\", \"SM\", \"[\", \"]\"]\n",
    "padding_index = len(tokens)\n",
    "lines = open(\"train_preprocessed.txt\").readlines()\n",
    "\n",
    "Y = []\n",
    "X = []\n",
    "for line in lines:\n",
    "    y, x = line.split(\",\")\n",
    "    y = int(y)\n",
    "    x = [tokens.index(t) for t in x.strip().split(\" \")]\n",
    "    Y.append(y)\n",
    "    X.append(x)\n",
    "\n",
    "I = [i for i, x in enumerate(X) if len(x) <= 32]\n",
    "XX = []\n",
    "YY = torch.tensor([Y[i] for i in I])\n",
    "for i in I:\n",
    "    XX.append([*[16]*(32 - len(X[i])), *X[i]])\n",
    "XX = torch.tensor(XX)\n",
    "XX = F.one_hot(XX).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = torch.utils.data.TensorDataset(XX, YY)\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 32, 17]), torch.Size([64]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = torch.nn.GRU(17, 128, batch_first=True)\n",
    "lin = torch.nn.Linear(128, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, h = gru(x)\n",
    "y_hat = lin(h.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3120, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.gru = torch.nn.GRU(input_size=17, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.lin = torch.nn.Linear(hidden_size, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, h = self.gru(x)\n",
    "        y_hat = self.lin(h)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch 100, loss 2.0359\n",
      "epoch 0, batch 200, loss 2.1141\n",
      "epoch 0, batch 300, loss 1.8010\n",
      "epoch 0, batch 400, loss 1.7544\n",
      "epoch 0, batch 500, loss 1.3858\n",
      "epoch 0, batch 600, loss 1.2675\n",
      "epoch 0, batch 700, loss 1.4372\n",
      "epoch 0, batch 800, loss 1.1133\n",
      "epoch 0, batch 900, loss 1.2422\n",
      "epoch 1, batch 100, loss 1.1496\n",
      "epoch 1, batch 200, loss 1.3715\n",
      "epoch 1, batch 300, loss 1.1647\n",
      "epoch 1, batch 400, loss 0.9206\n",
      "epoch 1, batch 500, loss 1.2092\n",
      "epoch 1, batch 600, loss 1.1762\n",
      "epoch 1, batch 700, loss 1.1974\n",
      "epoch 1, batch 800, loss 1.1496\n",
      "epoch 1, batch 900, loss 0.9355\n",
      "epoch 2, batch 100, loss 0.9140\n",
      "epoch 2, batch 200, loss 1.0696\n",
      "epoch 2, batch 300, loss 0.8702\n",
      "epoch 2, batch 400, loss 1.1084\n",
      "epoch 2, batch 500, loss 0.9034\n",
      "epoch 2, batch 600, loss 1.1110\n",
      "epoch 2, batch 700, loss 0.9939\n",
      "epoch 2, batch 800, loss 0.9626\n",
      "epoch 2, batch 900, loss 0.9408\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "model = MyModel(num_layers=2)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  losses = []\n",
    "  for batch, (x, y) in enumerate(train_dl):\n",
    "    y_hat = model(x)[-1]\n",
    "    loss = F.cross_entropy(y_hat, y)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    with torch.no_grad():\n",
    "      loss = loss.detach().clone()\n",
    "      losses.append(loss.item())\n",
    "      if (batch + 1) % 100 == 0:\n",
    "        print(f\"epoch {epoch}, batch {batch + 1}, loss {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
